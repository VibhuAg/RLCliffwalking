{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Blackjack-v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Action Space:\n",
    "\n",
    "There are two actions: stick (0), and hit (1).\n",
    "\n",
    "Observation Space:\n",
    "\n",
    "The observation consists of a 3-tuple containing: the player’s current sum, \n",
    "the value of the dealer’s one showing card (1-10 where 1 is ace), and whether\n",
    "the player holds a usable ace (0 or 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below is shit copied from past homeworks\n",
    "\n",
    "# Homework 2\n",
    "\n",
    "def get_policy(values, gamma, policy):\n",
    "  for s in range(env.observation_space.n): \n",
    "    q = np.zeros(env.action_space.n)\n",
    "\n",
    "    for a in range(env.action_space.n): \n",
    "      for neighbor in env.env.P[s][a]:\n",
    "        prob, s_prime, reward, _ = neighbor\n",
    "        q[a] += (prob * (reward + gamma * values[s_prime]))\n",
    "\n",
    "    policy[s] = np.argmax(q)\n",
    "    \n",
    "  return policy\n",
    "\n",
    "# iterates on values 1000 times\n",
    "def value_iteration(values, gamma): \n",
    "\n",
    "  for i in range(1000): \n",
    "    old_vals = np.copy(values)\n",
    "\n",
    "    for s in range(env.observation_space.n): \n",
    "\n",
    "      action_value = []\n",
    "\n",
    "      for a in range(env.action_space.n): \n",
    "\n",
    "        expected_reward = 0\n",
    "        expected_value = 0\n",
    "\n",
    "        for neighbor in env.env.P[s][a]:\n",
    "          prob, s_prime, reward, _ = neighbor\n",
    "\n",
    "          expected_reward += reward * prob\n",
    "          expected_value += old_vals[s_prime] * prob\n",
    "\n",
    "          action_value += [expected_reward + (gamma * expected_value)]\n",
    "      \n",
    "      values[s] = max(action_value)\n",
    "    \n",
    "  return values\n",
    "\n",
    "gamma = 1\n",
    "values = np.zeros(env.observation_space.n)\n",
    "policy = np.zeros(env.observation_space.n)\n",
    "\n",
    "values = value_iteration(values, gamma)\n",
    "policy = get_policy(values, gamma, policy)\n",
    "\n",
    "total_reward = 0\n",
    "\n",
    "for i in range(1000):\n",
    "\tobs = env.reset()\n",
    "\n",
    "\twhile True:\n",
    "\t\taction = policy[obs]\n",
    "\t\tobs, reward, done, info = env.step(action)\n",
    "\t\t\n",
    "\t\tif done:\n",
    "\t\t\ttotal_reward += reward\n",
    "\t\t\tbreak\n",
    "      \n",
    "print(\"Average reward: \", total_reward / 1000.0)\n",
    "\n",
    "\n",
    "# Homework 3\n",
    "\n",
    "obs = env.reset()\n",
    "values = [0.5,0.5,0.5,0.5,0.5,0,0.5,0,0.5,0.5,0.5,0,0,0.5,0.5,1]\n",
    "alpha = 1\n",
    "gamma = 1\n",
    "\n",
    "# runs 10000 episodes\n",
    "for i in range(10000): \n",
    "  done = False\n",
    "  state = 0\n",
    "\n",
    "  while True: \n",
    "    \n",
    "    # get random action\n",
    "    action = env.action_space.sample()\n",
    "\n",
    "    # R, s’ ← take action given by policy(current state)\n",
    "    s_, r, done, _ = env.step(action)\n",
    "\n",
    "    # v(s) ← v(s) + α * (R + γ*v(s’) - v(s))\n",
    "    values[state] += alpha * (r + gamma * values[s_] - values[state])\n",
    "\n",
    "    # s ← s’\n",
    "    state = s_\n",
    "\n",
    "    if done: \n",
    "      break\n",
    "      clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BlackjackEnv' object has no attribute 'P'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_32832/748461630.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mP\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'BlackjackEnv' object has no attribute 'P'"
     ]
    }
   ],
   "source": [
    "a = 1\n",
    "s = 2\n",
    "env.env.P[s][a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Steps for policy iteration\n",
    "# 1. Start with random policy\n",
    "# 2. Update values of every state with Bellman expectation equation\n",
    "# 3. Find new optimal policy be acting greedily on state values\n",
    "def get_policy(values, gamma, policy):\n",
    "  for s in range(len(env.observation_space)): \n",
    "    \n",
    "    # Action space is either 0 or 1\n",
    "    q = np.zeros(2)\n",
    "\n",
    "    for a in range(2): \n",
    "      for neighbor in env.env.P[s][a]:\n",
    "        prob, s_prime, reward, _ = neighbor\n",
    "        q[a] += (prob * (reward + gamma * values[s_prime]))\n",
    "\n",
    "    policy[s] = np.argmax(q)\n",
    "    \n",
    "  return policy\n",
    "\n",
    "# iterates on values 1000 times\n",
    "def value_iteration(values, gamma): \n",
    "\n",
    "  for i in range(1000): \n",
    "    old_vals = np.copy(values)\n",
    "\n",
    "    for s in range(env.observation_space.n): \n",
    "\n",
    "      action_value = []\n",
    "\n",
    "      for a in range(env.action_space.n): \n",
    "\n",
    "        expected_reward = 0\n",
    "        expected_value = 0\n",
    "\n",
    "        for neighbor in env.env.P[s][a]:\n",
    "          prob, s_prime, reward, _ = neighbor\n",
    "\n",
    "          expected_reward += reward * prob\n",
    "          expected_value += old_vals[s_prime] * prob\n",
    "\n",
    "          action_value += [expected_reward + (gamma * expected_value)]\n",
    "      \n",
    "      values[s] = max(action_value)\n",
    "    \n",
    "  return values\n",
    "\n",
    "gamma = 1\n",
    "values = np.zeros(env.observation_space.n)\n",
    "policy = np.zeros(env.observation_space.n)\n",
    "\n",
    "values = value_iteration(values, gamma)\n",
    "policy = get_policy(values, gamma, policy)\n",
    "\n",
    "total_reward = 0\n",
    "\n",
    "for i in range(1000):\n",
    "\tobs = env.reset()\n",
    "\n",
    "\twhile True:\n",
    "\t\taction = policy[obs]\n",
    "\t\tobs, reward, done, info = env.step(action)\n",
    "\t\t\n",
    "\t\tif done:\n",
    "\t\t\ttotal_reward += reward\n",
    "\t\t\tbreak\n",
    "      \n",
    "print(\"Average reward: \", total_reward / 1000.0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CliffWalking-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1.0, 1, -1, False)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.env.P[13][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stop here\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "x  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "x  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "stop here"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_25764/1230465865.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[0mpolicy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue_iteration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[0mpolicy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_policy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_25764/1230465865.py\u001b[0m in \u001b[0;36mvalue_iteration\u001b[1;34m(values, gamma)\u001b[0m\n\u001b[0;32m     22\u001b[0m       \u001b[0maction_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m34\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"stop here\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m       \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\ipykernel\\iostream.py\u001b[0m in \u001b[0;36mwrite\u001b[1;34m(self, string)\u001b[0m\n\u001b[0;32m    527\u001b[0m             \u001b[0mis_child\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_master_process\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    528\u001b[0m             \u001b[1;31m# only touch the buffer in the IO thread to avoid races\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 529\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    530\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_child\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m                 \u001b[1;31m# mp.Pool cannot be trusted to flush promptly (or ever),\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\ipykernel\\iostream.py\u001b[0m in \u001b[0;36mschedule\u001b[1;34m(self, f)\u001b[0m\n\u001b[0;32m    212\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_events\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m             \u001b[1;31m# wake event thread (message content is ignored)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 214\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_event_pipe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mb''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    215\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m             \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\zmq\\sugar\\socket.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, data, flags, copy, track, routing_id, group)\u001b[0m\n\u001b[0;32m    539\u001b[0m                 )\n\u001b[0;32m    540\u001b[0m             \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSocket\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msend_multipart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg_parts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._send_copy\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\zmq\\backend\\cython\\checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def get_policy(values, gamma, policy):\n",
    "  for s in range(env.observation_space.n): \n",
    "    q = np.zeros(env.action_space.n)\n",
    "\n",
    "    for a in range(env.action_space.n): \n",
    "      for neighbor in env.env.P[s][a]:\n",
    "        prob, s_prime, reward, _ = neighbor\n",
    "        q[a] += (prob * (reward + gamma * values[s_prime]))\n",
    "\n",
    "    policy[s] = np.argmax(q)\n",
    "    \n",
    "  return policy\n",
    "\n",
    "# iterates on values 1000 times\n",
    "def value_iteration(values, gamma): \n",
    "\n",
    "  for i in range(100): \n",
    "    old_vals = np.copy(values)\n",
    "\n",
    "    for s in range(env.observation_space.n): \n",
    "\n",
    "      action_value = []\n",
    "      if s == 34:\n",
    "        print(\"stop here\")\n",
    "      for a in range(env.action_space.n): \n",
    "\n",
    "        expected_reward = 0\n",
    "        expected_value = 0\n",
    "        neighbor = env.env.P[s][a]\n",
    "        prob, s_prime, reward, _ = neighbor[0]\n",
    "\n",
    "        expected_reward += reward\n",
    "        expected_value += old_vals[s_prime] * prob\n",
    "\n",
    "        action_value += [expected_reward + (gamma * expected_value)]\n",
    "      \n",
    "      values[s] = max(action_value)\n",
    "    \n",
    "  return values\n",
    "\n",
    "gamma = 0.7\n",
    "values = np.zeros(env.observation_space.n)\n",
    "policy = np.zeros(env.observation_space.n)\n",
    "\n",
    "values = value_iteration(values, gamma)\n",
    "print(values)\n",
    "policy = get_policy(values, gamma, policy)\n",
    "print(policy)\n",
    "total_reward = 0\n",
    "\n",
    "for i in range(1):\n",
    "    obs = env.reset()\n",
    "\n",
    "    while True:\n",
    "        action = policy[obs]\n",
    "        print(action)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "\n",
    "        if done:\n",
    "            total_reward += reward\n",
    "            break\n",
    "      \n",
    "print(\"Average reward: \", total_reward / 1.0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-120\n",
      "-1545\n",
      "-127\n",
      "-80\n",
      "-101\n",
      "-47\n",
      "-88\n",
      "-52\n",
      "-56\n",
      "-69\n",
      "-72\n",
      "-61\n",
      "-44\n",
      "-29\n",
      "-75\n",
      "-41\n",
      "-43\n",
      "-67\n",
      "-31\n",
      "-37\n",
      "-24\n",
      "-42\n",
      "-72\n",
      "-35\n",
      "-33\n",
      "-49\n",
      "-42\n",
      "-18\n",
      "-21\n",
      "-25\n",
      "-37\n",
      "-42\n",
      "-31\n",
      "-25\n",
      "-13\n",
      "-32\n",
      "-20\n",
      "-37\n",
      "-20\n",
      "-27\n",
      "-30\n",
      "-22\n",
      "-25\n",
      "-20\n",
      "-27\n",
      "-24\n",
      "-27\n",
      "-15\n",
      "-24\n",
      "-19\n",
      "-15\n",
      "-17\n",
      "-21\n",
      "-22\n",
      "-15\n",
      "-15\n",
      "-22\n",
      "-13\n",
      "-13\n",
      "-16\n",
      "-27\n",
      "-15\n",
      "-13\n",
      "-17\n",
      "-13\n",
      "-13\n",
      "-13\n",
      "-13\n",
      "-24\n",
      "-13\n",
      "-13\n",
      "-13\n",
      "-13\n",
      "-13\n",
      "-13\n",
      "-13\n",
      "-13\n",
      "-13\n",
      "-13\n",
      "-13\n",
      "-13\n",
      "-13\n",
      "-13\n",
      "-13\n",
      "-13\n",
      "-13\n",
      "-13\n",
      "-13\n",
      "-13\n",
      "-13\n",
      "-13\n",
      "-13\n",
      "-13\n",
      "-13\n",
      "-13\n",
      "-13\n",
      "-13\n",
      "-13\n",
      "-13\n",
      "-13\n"
     ]
    }
   ],
   "source": [
    "q_table = np.zeros(([env.observation_space.n, env.action_space.n]))\n",
    "alpha = 0.5\n",
    "gamma = 0.9\n",
    " # start state\n",
    "for i in range(100):\n",
    "    old_obs = 36\n",
    "    env.reset()\n",
    "    total_reward = 0\n",
    "    while True:\n",
    "        max_action = np.argmax(q_table[old_obs, :])\n",
    "        obs, reward, done, info = env.step(max_action)\n",
    "        total_reward += reward\n",
    "        action_prime = np.argmax(q_table[obs, :])\n",
    "        q_table[old_obs, max_action] += alpha*(reward + gamma*(q_table[obs, action_prime]- q_table[old_obs, max_action]))\n",
    "        old_obs = obs\n",
    "        if done:\n",
    "            break\n",
    "    print(total_reward)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2be5faf79681da6f2a61fdfdd5405d65d042280f7fba6178067603e3a2925119"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
